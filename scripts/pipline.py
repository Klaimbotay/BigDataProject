# -*- coding: utf-8 -*-
"""BD_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17LZy-rgih5mgyaNFrqNjQ4TidzSevFck
"""

# !pip install pyspark

import numpy as np
import matplotlib.pyplot as plt
from pyspark.sql.functions import col
from pyspark.sql import SparkSession, SQLContext
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType
from pyspark.ml.feature import StringIndexer, VectorAssembler, PCA, Imputer
from pyspark.ml.functions import vector_to_array
from pyspark.ml.clustering import KMeans, BisectingKMeans
from pyspark.ml.evaluation import ClusteringEvaluator

"""Load data"""

spark = SparkSession \
        .builder \
        .appName("my spark app") \
        .master("local[*]") \
        .getOrCreate()

sc = spark.sparkContext

schema_data = StructType([
    StructField("id", IntegerType(), False),
    StructField("year", IntegerType(), False),
    StructField("month", IntegerType(), False),
    StructField("day", IntegerType(), False),
    StructField("hour", IntegerType(), False),
    StructField("PM25", FloatType(), False),
    StructField("PM10", FloatType(), False),
    StructField("SO2", FloatType(), False),
    StructField("NO2", FloatType(), False),
    StructField("CO", FloatType(), False),
    StructField("O3", FloatType(), False),
    StructField("TEMP", FloatType(), False),
    StructField("PRES", FloatType(), False),
    StructField("DEWP", FloatType(), False),
    StructField("RAIN", FloatType(), False),
    StructField("wd", StringType(), False),
    StructField("WSPM", FloatType(), False),
    StructField("station", StringType(), False),
])

path = '../data/'

df = spark.read.format("csv") \
  .option("sep", ",") \
  .option("inferSchema", "true") \
  .option("header", "true") \
  .load(path+'project_tb.csv',header=True, schema = schema_data, escape='"')

# df.show()

"""Look at data"""

# df.describe().show()

mean_cols = ['PM25', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']
freq_cols = ['wd']
# Imputing characteristics columns with mean, but wind_direction with most frequent 
imputer = Imputer()
imputer.setInputCols(mean_cols)
imputer.setOutputCols(mean_cols)
df = imputer.setStrategy("mean").fit(df).transform(df)

most_freq = df.groupBy('wd').count().orderBy('count',ascending=False).first()[0]
df = df.na.fill(value=most_freq, subset=freq_cols)

"""Now there are no empty cells"""

# df.describe().show()

"""EDA:Show balance between sources, mean temperature and frequency of wind directions."""

df.groupBy('station').count().write.mode("overwrite").format("csv").save("../outputs/station_balance")
# df.groupBy('station').count().show()

df.groupBy('station').mean('TEMP').orderBy('avg(TEMP)').write.mode("overwrite").format("csv").save("../outputs/station_meantemp")
# df.groupBy('station').mean('TEMP').orderBy('avg(TEMP)').show()

df.groupBy(['year', 'wd']).count().orderBy(col("year").asc(),col("count").desc()).write.mode("overwrite").format("csv").save("../outputs/year_wdcount")
# df.groupBy(['year', 'wd']).count().orderBy(col("year").asc(),col("count").desc()).show()

"""We need to be sure, that series go in right order"""

df = df.sort(['station', 'year','month','day','hour'])

"""Change strings to numbers"""

stringIndexer = StringIndexer(inputCol="wd", outputCol="encoded_wd")
model = stringIndexer.fit(df)
encoded = model.transform(df)

"""Everything sorted, now we can drop time data and strings"""

encoded_df = encoded.drop('wd', 'id', 'year', 'month', 'day', 'hour')

# encoded_df.show()

"""To reduce dimentionality need to create vectors, after that get df with station and one component that characterizes moment in time."""

pca_attributes = ['PM25', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM', 'encoded_wd']
assembler = VectorAssembler(
    inputCols=pca_attributes,
    outputCol="features"
)
output = assembler.transform(encoded_df)

pca = PCA(k=1, inputCol="features", outputCol="pcaFeatures")
model = pca.fit(output)
pcares = model.transform(output)

final_df = pcares.select('pcaFeatures', 'station')
final_df = final_df.withColumn("cluster", vector_to_array("pcaFeatures")).select(["station"] + [col("cluster")[0]])
final_df=final_df.withColumnRenamed("cluster[0]","cluster_feature")

# final_df.show()

"""Create slides (vectors that show period) is an approach to clusterize timeseries data, do this, but for each station separately"""

stations = final_df.select('station').distinct().rdd.map(lambda x: x.station).collect()

def insequences(s_df):
  s_array = np.array(s_df.collect())
  l = len(s_array)
  rows = l//24
  result = np.zeros([1,24])
  for i in range(rows):
    part = s_array[24*i:24*(i+1)]
    part = part.T
    result = np.vstack((result, part))
  return result

"""Do this for every station and concat, now we do not need stations, we have vectors that characterize periods, let's clusterize them"""

slides = np.zeros([1,24])

for s in stations:
  s_df = final_df.select('cluster_feature').filter(final_df.station == s)
  s_seq = insequences(s_df)
  s_seq = np.delete(s_seq, 0, 0)
  slides = np.vstack((slides, s_seq))

slides = np.delete(slides, 0, 0)
mydf = spark.createDataFrame(slides)

att = mydf.columns
assembler = VectorAssembler(
    inputCols=att,
    outputCol="features"
)
mydf = assembler.transform(mydf)
mydf = mydf.drop(*att)

# mydf.show()

"""I use k-means model and Bisecting k-means"""

def clustering_kmeans(df, k, seed):
  model = KMeans().setK(k).setSeed(seed)
  model = model.fit(df)

  # Make predictions
  predictions = model.transform(df)

  # Evaluate clustering by computing Silhouette score
  evaluator = ClusteringEvaluator()

  kmeans_silhouette = evaluator.evaluate(predictions)
  # print("Silhouette with squared euclidean distance = " + str(silhouette))
  kmeans_centers = model.clusterCenters()
  return kmeans_silhouette, kmeans_centers, model


def clustering_bkmeans(df, k, seed):
  model = BisectingKMeans().setK(k).setSeed(seed)
  model = model.fit(df)

  # Make predictions
  predictions = model.transform(df)

  # Evaluate clustering by computing Silhouette score
  evaluator = ClusteringEvaluator()

  bkmeans_silhouette = evaluator.evaluate(predictions)
  bkmeans_centers = model.clusterCenters()

  return bkmeans_silhouette, bkmeans_centers, model

"""Check for 2 to 5 clusters and different seeds"""

results_seed1 = []
for k in range(2,6):
  kmeans_silhouette, kmeans_centers, model = clustering_kmeans(mydf, k, 1)
  results_seed1.append(kmeans_silhouette)

results_seed2 = []
for k in range(2,6):
  kmeans_silhouette, kmeans_centers, model = clustering_kmeans(mydf, k, 2)
  results_seed2.append(kmeans_silhouette)

results_seed3 = []
for k in range(2,6):
  kmeans_silhouette, kmeans_centers, model = clustering_kmeans(mydf, k, 3)
  results_seed3.append(kmeans_silhouette)

plt.plot([k for k in range(2,6)], results_seed1)
plt.title('KMeans')
plt.xlabel('Clusters')
plt.ylabel('Silhouette')
plt.savefig('../outputs/models_plots/kmeans1.png')
# plt.show()

plt.plot([k for k in range(2,6)], results_seed2)
plt.title('KMeans')
plt.xlabel('Clusters')
plt.ylabel('Silhouette')
plt.savefig('../outputs/models_plots/kmeans2.png')
# plt.show()

plt.plot([k for k in range(2,6)], results_seed3)
plt.title('KMeans')
plt.xlabel('Clusters')
plt.ylabel('Silhouette')
plt.savefig('../outputs/models_plots/kmeans3.png')
# plt.show()

results_seed1 = []
for k in range(2,6):
  bkmeans_silhouette, bkmeans_centers, model = clustering_bkmeans(mydf, k, 1)
  results_seed1.append(bkmeans_silhouette)

results_seed2 = []
for k in range(2,6):
  bkmeans_silhouette, bkmeans_centers, model = clustering_bkmeans(mydf, k, 2)
  results_seed2.append(bkmeans_silhouette)

results_seed3 = []
for k in range(2,6):
  bkmeans_silhouette, bkmeans_centers, model = clustering_bkmeans(mydf, k, 3)
  results_seed3.append(bkmeans_silhouette)

plt.plot([k for k in range(2,6)], results_seed1)
plt.title('BisectingKMeans')
plt.xlabel('Clusters')
plt.ylabel('Silhouette')
plt.savefig('../outputs/models_plots/bkmeans1.png')
# plt.show()

plt.plot([k for k in range(2,6)], results_seed2)
plt.title('BisectingKMeans')
plt.xlabel('Clusters')
plt.ylabel('Silhouette')
plt.savefig('../outputs/models_plots/bkmeans2.png')
# plt.show()

plt.plot([k for k in range(2,6)], results_seed3)
plt.title('BisectingKMeans')
plt.xlabel('Clusters')
plt.ylabel('Silhouette')
plt.savefig('../outputs/models_plots/bkmeans3.png')
# plt.show()

"""Plots shows, that 2 clusters is the best number, of course, only a specialist can evaluate what kind of groups turned out and which of them is good and which is bad air. But I help to create a way to accurately determine the group to which air belongs.

I can not save model, so it's not necessary.
In future, to decide what kind of air we have, we need only to reduce features, create slides and calculate to what center coords they are closer.
"""

kmeans_silhouette, kmeans_centers, model = clustering_kmeans(mydf, 2, 1)
ans = f'This info saved to folder "models"\nBest number of KMeans clusters is 2, silhouette of square euqlidean distance = {kmeans_silhouette} with any seed, centers = {kmeans_centers}'
print(ans)
with open('../models/model_info.txt', 'w') as f:
    f.write(ans)

# !pip freeze > requirements.txt